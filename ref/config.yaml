# Default experiment config
vocab_size: 128000
d_model: 4096
n_layers: 8
n_heads: 32
n_kv_heads: 8
d_ff: 14336
attn_type: mla         # standard | gqa | mla
mla_rq: 1024
mla_rkv: 512
dropout: 0.0
tie_embeddings: true

# mask-diffusion token masking policy
mask_fraction_min: 0.15
mask_fraction_max: 0.90

# training
lr: 1.0e-4
weight_decay: 0.1
betas: [0.9, 0.95]
batch_size: 4
seq_len: 512
max_steps: 200
device: cuda
