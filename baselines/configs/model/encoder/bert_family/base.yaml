# @package model

# BERT Base configuration (110M parameters)
# Target: BERT-base-uncased reproduction

# Model architecture
d_model: 768
n_heads: 12
n_kv_heads: 12
d_ff: 3072
n_layers: 12
vocab_size: 30522

# Attention configuration
attention_preset: "bert_style"
attention_mode: "bidirectional"

# Normalization and MLP
dropout: 0.1
tie_embeddings: true
freeze_up_proj: false

# Positional encoding
use_rope: false
rope_theta: 10000.0

# Mask-diffusion parameters
mask_fraction_min: 0.15
mask_fraction_max: 0.90
special_mask_id: 103  # [MASK] token ID for BERT

# Tokenizer and embedding configuration
tokenizer_embedding:
  family: "bert"
  model_name: "bert-base-uncased"
  embedding_option: "embed_tokens"
  freeze_embeddings: false
  adapter_hidden_dim: null

# Architecture configuration
architecture:
  n_stacks: 12
  blocks_per_stack: 1

# Target parameters for validation
target_params: 110_000_000
target_flops: 2.3e12  # Approximate FLOPs for BERT-base
