# @package training

# Classical training configuration
# Standard end-to-end training without progressive building

# Training parameters
batch_size: 16
seq_len: 512
max_steps: 10000
learning_rate: 2e-5
weight_decay: 0.01
warmup_steps: 1000
num_epochs: 3

# Optimizer configuration
optimizer:
  optimizer_type: "AdamW"
  lr: 2e-5
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8

# Device and memory
device: "cuda"
gradient_checkpointing: false
mixed_precision: true

# Training strategy
strategy: "end_to_end"
end_to_end_scope: "rackwise"

# Progressive training (disabled for classical)
progressive:
  enabled: false
  max_stacks: 12
  target_stacks: 12
  building_mode: "append"
  trunk_strategy: "frozen"
  new_stack_precision: "full"
  cache_activations: false
  time_interpretation: "depth"
  training_objective: "mlm"

# QLoRA settings (disabled for classical)
qlora:
  enabled: false
  rank: 16
  alpha: 32
  dropout: 0.1
  lr: 1e-5
  strategy: "simplified"
  mixed_precision: false

# Logging and checkpointing
log_interval: 100
save_interval: 1000
checkpoint_dir: "./checkpoints"

# Weights & Biases
use_wandb: true
wandb_project: "stackwise-baselines"
wandb_entity: null
wandb_run_name: null
wandb_tags: ["classical", "reproduction"]
wandb_notes: "Classical training reproduction"
