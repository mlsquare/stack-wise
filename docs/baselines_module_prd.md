# Baselines Module PRD: Encoderâ€“Decoder Families & Scaling-Benchmark Framework

---

## 1ï¸âƒ£ Objective

Establish a **baseline and scaling benchmark module** within the main framework (`baselines/`) to validate the *depth-as-time* and *curriculum-flexible* architecture against well-known encoder (BERT-family) and decoder (GPT-family) models under comparable compute budgets.

The module enables reproducible scaling, benchmarking, and cross-family evaluation at multiple model sizes â€” from Tiny to Large.

---

## 2ï¸âƒ£ Scope

| Family | Type | Reference Models | Goal |
|--------|------|------------------|------|
| **Encoders** | Bidirectional / Masked | Tiny-BERT, MiniLM, DistilBERT, ModernBERT, BERT-base/large | Evaluate embedding quality & NLU tasks |
| **Decoders** | Autoregressive / Causal | Tiny-GPT, GPT-2 (124Mâ€“1.5B), LLaMA family | Evaluate text generation, reasoning, perplexity |
| **Unified / Diffusion** | Mask-Diffusion (depth-as-time) | Compare scaling laws vs classical baselines |

---

## 3ï¸âƒ£ Design Goals

1. **Fair scaling** â€” match models by compute/token budgets, not just parameter count.  
2. **Composable configs** â€” per-family YAML configs defining width, depth, heads, FFN ratio, mask ratio.  
3. **Unified trainers** â€” both *classical* and *depth-as-time* training regimes selectable via flag.  
4. **Standardized benchmarks** â€” unified evaluation harness for NLU and NLG.  
5. **Extensible registry** â€” easy to add new model families, configs, and benchmarks.

---

## 4ï¸âƒ£ Directory Structure

```
/baselines/
 â”œâ”€â”€ encoder/
 â”‚   â”œâ”€â”€ bert_family/
 â”‚   â”‚   â”œâ”€â”€ tiny.yaml
 â”‚   â”‚   â”œâ”€â”€ base.yaml
 â”‚   â”‚   â”œâ”€â”€ large.yaml
 â”‚   â”‚   â””â”€â”€ modern.yaml
 â”‚   â””â”€â”€ modernbert_family/
 â”œâ”€â”€ decoder/
 â”‚   â”œâ”€â”€ gpt2_family/
 â”‚   â”‚   â”œâ”€â”€ nano.yaml
 â”‚   â”‚   â”œâ”€â”€ small.yaml
 â”‚   â”‚   â”œâ”€â”€ medium.yaml
 â”‚   â”‚   â”œâ”€â”€ large.yaml
 â”‚   â”‚   â””â”€â”€ xl.yaml
 â”‚   â””â”€â”€ llama_family/
 â”œâ”€â”€ trainer/
 â”‚   â”œâ”€â”€ classical_trainer.py
 â”‚   â””â”€â”€ depth_time_trainer.py
 â”œâ”€â”€ benchmarks/
 â”‚   â”œâ”€â”€ datasets.yaml
 â”‚   â””â”€â”€ evaluate.py
 â””â”€â”€ configs/
     â””â”€â”€ scaling/
         â”œâ”€â”€ nano.yaml
         â”œâ”€â”€ small.yaml
         â”œâ”€â”€ base.yaml
         â”œâ”€â”€ large.yaml
         â””â”€â”€ xl.yaml
```

---

## 5ï¸âƒ£ Training Regimes

| Regime | Description |
|---------|-------------|
| **Classic Pre-training** | Standard masked LM (encoder) or causal LM (decoder). |
| **Depth-as-Time** | Layer-wise diffusion-style denoising curriculum (Rightâ†’Left). |
| **Hybrid** | Fine-tune classical checkpoint under diffusion curriculum. |

Command-line usage:
```
train.py --family bert --config base.yaml --curriculum right2left --noise-schedule cosine
```

---

## 6ï¸âƒ£ Model Scaling

| Scale | Params | Layers | d_model | FFN ratio | Purpose |
|-------|---------|---------|----------|------------|----------|
| Nano | 15M | 4 | 256 | 2 | Sanity checks |
| Tiny | 40M | 6 | 384 | 3 | Light prototype |
| Small | 80M | 12 | 512 | 4 | Tiny-BERT / GPT-2-small equivalent |
| Base | 120M | 12 | 768 | 4 | Reference baseline |
| Large | 350Mâ€“1B | 24â€“36 | 1024â€“1536 | 4 | High-capacity proof |
| XL | 1â€“3B+ | 48+ | 2048+ | 4 | Stretch-goal (HPC) |

---

## 7ï¸âƒ£ Benchmarks

### Encoder Tasks (NLU)

| Task | Dataset | Metric |
|-------|----------|--------|
| Semantic Textual Similarity | STS-B | Spearman / Pearson |
| Natural Language Inference | MNLI, SNLI | Accuracy |
| Question Answering | SQuAD v1/v2 | F1 / EM |
| Sentiment | SST-2 | Accuracy |
| Named Entity Recognition | CoNLL-2003 | F1 |

### Decoder Tasks (NLG)

| Task | Dataset | Metric |
|-------|----------|--------|
| Language Modeling | WikiText-103 | Perplexity |
| QA / Completion | LAMBADA | Accuracy |
| Reasoning | PIQA, HellaSwag | Accuracy |
| Summarization | CNN/DailyMail | ROUGE-L |
| Translation | WMT-14 (en-de) | BLEU |

---

## 8ï¸âƒ£ Compute & Budgeting

- Equalize **training FLOPs**: `FLOPs = tokens Ã— FLOPs/token Ã— epochs`  
- Define per-scale compute targets.  
- Auto-adjust steps and batch size to maintain equal compute.  
- Support mixed precision (bf16/fp16) via `accelerate` or `DeepSpeed`.

---

## 9ï¸âƒ£ Evaluation Harness

```
evaluate.py --model bert-base --tasks mnli,sst2,stsb
evaluate.py --model gpt2-small --tasks wikitext103,hellaswag
```
Features:
- Auto dataset loading (via HuggingFace `datasets`).  
- Tokenizer auto-select by family.  
- Unified metric interface.  
- Logs to TensorBoard / Weights & Biases.  
- Supports zero-shot + fine-tuned eval.  
- Embedding extraction API for STS tasks.

---

## ğŸ”Ÿ Integration with Depth-as-Time

- Compatible with the `DenoiserStack` architecture.  
- Encoder families can use *mask-diffusion objectives* (conditional denoising).  
- Decoder families can use *autoregressive unmasking schedules*.  
- Depth-as-time mode implemented as `depth_time_trainer.py` in `/trainer/`.

---

## 11ï¸âƒ£ Milestones

| Phase | Deliverable | Duration |
|-------|--------------|----------|
| **P1** | Tiny-BERT & Tiny-GPT prototypes, single-node runs | 2â€“3 weeks |
| **P2** | Base & Small models across both families, full benchmarks | 4â€“6 weeks |
| **P3** | Integrate depth-as-time variants, scaling-law plots | 3 weeks |
| **P4** | Comparative report: compute vs performance | 2 weeks |

---

## 12ï¸âƒ£ Metrics of Success

- Tiny/Base models achieve within **5â€“10%** of standard baselines (BERT-base, GPT-2-small).  
- Scaling-law exponents consistent with known literature (Î± â‰ˆ 0.05â€“0.10).  
- Depth-as-time variants maintain comparable downstream performance with reduced compute.  

---

## 13ï¸âƒ£ Repository Decision

Start as **`baselines/` module inside main framework**, not a separate repo.

**Pros:**
- Reuse shared components (`DenoiserStack`, trainers, datasets).  
- Easier integrated experiments across curricula.  
- Central logging & evaluation utilities.

**Later Option:**
Export as standalone repo (`model-baselines/`) for publication or public benchmarking once stable.

---

## âœ… Summary

This module ensures:
- Comparable encoder & decoder baselines under equal compute.  
- Joint benchmarking for *classical* and *depth-as-time* regimes.  
- Unified configuration, dataset, and evaluation registry.  
- Reproducible scaling studies to validate model performance across scales.

---

*(End of PRD)*
