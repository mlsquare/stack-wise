# StackWise Configuration
# Hierarchical configuration with validation and defaults

model:
  # Model dimensions
  vocab_size: null  # Will be set from tokenizer
  d_model: 4096
  n_heads: 32
  n_kv_heads: 8
  d_ff: 14336
  
  # Architecture configuration (new hierarchical structure)
  # Block: Standard transformer block (attention + FFN + layer norm + residual)
  # Stack: Collection of multiple blocks
  # Rack: Final model containing multiple stacks
  architecture:
    n_stacks: 2        # Number of stacks (groups of blocks)
    blocks_per_stack: 4  # Number of blocks per stack
    
  # DEPRECATED: Use architecture.n_stacks and architecture.blocks_per_stack instead
  # n_layers: 8  # DEPRECATED - use architecture configuration
  
  # Attention configuration
  attention_type: "mha"              # mha | gqa | mla | kernel
  attention_mode: "bidirectional"   # bidirectional | causal
  
  # MLA specific parameters
  mla_rq: 1024
  mla_rkv: 512
  
  # Kernel attention parameters
  kernel_type: "gaussian"           # gaussian | laplacian | uniform
  kernel_dim: 64
  
  # Normalization and MLP
  dropout: 0.0
  tie_embeddings: true
  
  # Positional encoding
  use_rope: true
  rope_theta: 10000.0
  
  # Mask-diffusion parameters
  mask_fraction_min: 0.15
  mask_fraction_max: 0.90
  special_mask_id: 4
  
  # Tokenizer and embedding configuration
  tokenizer_embedding:
    family: "gpt2"
    embedding_option: "embed_tokens"
    freeze_embeddings: true
    adapter_hidden_dim: null

training:
  # Training parameters
  lr: 1.0e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]
  batch_size: 4
  seq_len: 512
  max_steps: 200
  
  # Device and memory
  device: "cuda"
  gradient_checkpointing: false
  mixed_precision: false
  
  # Layer-wise training
  layerwise_training: true
  activation_cache_dir: "./cache"
  save_activations: true
  
  # Caching configuration
  cache_mode: "layerwise"          # layerwise | fusion
  fusion_evaluation: false
  save_fused_checkpoints: false
  
  # Mask-diffusion training
  min_mask_fraction: 0.15
  max_mask_fraction: 0.90
  mask_schedule_type: "linear"      # linear | exponential | cosine
  mask_token_id: 0
  epochs_per_layer: 1
  learning_rate: 1.0e-4
  
  # New training modes (updated naming)
  mode: "layerwise"                 # layerwise | blockwise | fused
  block_size: 4                     # Number of blocks per stack
  fusion_mode: "frozen"             # frozen | trainable (for fused mode)
  
  # Architecture training modes
  training_architecture: "blockwise"  # blockwise | stackwise | rackwise
  # blockwise: Train each block independently
  # stackwise: Train each stack independently  
  # rackwise: Train the entire rack (all stacks together)
  
  # Fusion and fine-tuning
  fusion_enabled: true
  joint_tuning_steps: 50
  fine_tune_mode: "clm"             # clm | mlm | diffusion
  
  # Run identification and organization
  run_id: "default_run"            # Unique identifier for this training run
  total_blocks: 2                   # Total number of blocks to train
  
  # QLoRA and quantization settings
  qlora_enabled: true              # Enable QLoRA adapters
  qlora_lr: 1.0e-5                 # Learning rate for QLoRA parameters
  current_block_lr: 1.0e-4         # Learning rate for current block
  quantization_enabled: true        # Enable quantization
  quantization_type: "fp16"         # fp4 | fp8 | fp16 | fp32
  mixed_precision: true             # Use mixed precision training
  
  # Time-step-based masking
  time_step_masking: true           # Enable time-step-based masking
  num_time_steps: 8                 # Number of discrete time steps
  time_step_mask_fractions: [0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85]
  
  # Progressive training configuration
  progressive:
    enabled: true                   # Enable progressive training
    trunk_strategy: "frozen"        # "frozen" or "qlora"
    new_stack_precision: "full"     # "full", "half", "bfloat16", "qlora"
    cache_activations: true         # Cache activations for trunk training
    
    # Dual-LoRA configuration
    qlora_enabled: true             # Enable QLoRA adapters
    qlora_strategy: "progressive"   # "simplified", "progressive", "variable"
    
    # Stack LoRA parameters (added to each stack)
    qlora_rank: 16                  # Base QLoRA rank
    qlora_alpha: 32                 # Base QLoRA alpha
    
    # Progressive QLoRA parameters (added to entire trunk when new stack is added)
    progressive_qlora: true               # Enable progressive QLoRA adapters
    progressive_qlora_rank: 8             # Progressive QLoRA rank (smaller than stack LoRA)
    progressive_qlora_alpha: 16           # Progressive QLoRA alpha
    
    # Progressive patterns (for qlora_strategy: "progressive")
    qlora_rank_pattern: "increasing" # "constant", "increasing", "decreasing", "linear"
    qlora_alpha_pattern: "constant" # "constant", "increasing", "decreasing", "linear"
    
    # Variable QLoRA configurations (for qlora_strategy: "variable")
    qlora_configs:
      0: {rank: 8, alpha: 16}       # Stack 0: small QLoRA
      1: {rank: 16, alpha: 32}      # Stack 1: medium QLoRA  
      2: {rank: 32, alpha: 64}      # Stack 2: large QLoRA
      3: {rank: 64, alpha: 128}     # Stack 3: very large QLoRA
    
    max_stacks: 12                  # Maximum number of stacks
    building_mode: "append"         # "append" or "prepend"
    
    # Training objective
    training_objective: "mlm"       # "mlm", "clm", "custom"
    
    # Time interpretation
    time_interpretation: "depth"    # "input" or "depth"
    time_embedding_dim: 512        # Time embedding dimension (time-as-input)
    time_encoding_type: "sinusoidal" # "sinusoidal" or "learned"
    stack_time_mapping: "linear"    # "linear", "exponential", "custom"
    time_per_stack: 100             # Time steps per stack (time-as-depth)
  
  # Logging and checkpointing
  log_interval: 10
  save_interval: 100
  checkpoint_dir: "./checkpoints"

data:
  # Dataset parameters
  dataset_path: null
  use_dummy_data: true
  num_samples: 128
  
  # Data preprocessing
  tokenizer_path: null
  max_length: 512
  padding: "right"
  
  # Data loading
  num_workers: 0
  pin_memory: true
  shuffle: true
