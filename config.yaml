# StackWise Configuration
# Hierarchical configuration with validation and defaults

model:
  # Model dimensions
  vocab_size: null  # Will be set from tokenizer
  d_model: 4096
  n_layers: 8
  n_heads: 32
  n_kv_heads: 8
  d_ff: 14336
  
  # Attention configuration
  attention_type: "standard"        # standard | gqa | mla | kernel
  attention_mode: "bidirectional"   # bidirectional | causal
  
  # MLA specific parameters
  mla_rq: 1024
  mla_rkv: 512
  
  # Kernel attention parameters
  kernel_type: "gaussian"           # gaussian | laplacian | uniform
  kernel_dim: 64
  
  # Normalization and MLP
  dropout: 0.0
  tie_embeddings: true
  
  # Positional encoding
  use_rope: true
  rope_theta: 10000.0
  
  # Mask-diffusion parameters
  mask_fraction_min: 0.15
  mask_fraction_max: 0.90
  special_mask_id: 4
  
  # Tokenizer and embedding configuration
  tokenizer_embedding:
    family: "gpt2"
    embedding_option: "embed_tokens"
    freeze_embeddings: true
    adapter_hidden_dim: null

training:
  # Training parameters
  lr: 1.0e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]
  batch_size: 4
  seq_len: 512
  max_steps: 200
  
  # Device and memory
  device: "cuda"
  gradient_checkpointing: false
  mixed_precision: false
  
  # Layer-wise training
  layerwise_training: true
  activation_cache_dir: "./cache"
  save_activations: true
  
  # Caching configuration
  cache_mode: "layerwise"          # layerwise | fusion
  fusion_evaluation: false
  save_fused_checkpoints: false
  
  # Mask-diffusion training
  min_mask_fraction: 0.15
  max_mask_fraction: 0.90
  mask_schedule_type: "linear"      # linear | exponential | cosine
  mask_token_id: 0
  epochs_per_layer: 1
  learning_rate: 1.0e-4
  
  # New training modes
  mode: "layerwise"                 # layerwise | blockwise | fused
  block_size: 4                     # Number of layers per block
  fusion_mode: "frozen"             # frozen | trainable (for fused mode)
  
  # Fusion and fine-tuning
  fusion_enabled: true
  joint_tuning_steps: 50
  fine_tune_mode: "clm"             # clm | mlm | diffusion
  
  # Run identification and organization
  run_id: "default_run"            # Unique identifier for this training run
  total_blocks: 2                   # Total number of blocks to train
  
  # QLoRA and quantization settings
  qlora_enabled: true              # Enable QLoRA adapters
  qlora_lr: 1.0e-5                 # Learning rate for QLoRA parameters
  current_block_lr: 1.0e-4         # Learning rate for current block
  quantization_enabled: true        # Enable quantization
  quantization_type: "fp16"         # fp4 | fp8 | fp16 | fp32
  mixed_precision: true             # Use mixed precision training
  
  # Time-step-based masking
  time_step_masking: true           # Enable time-step-based masking
  num_time_steps: 8                 # Number of discrete time steps
  time_step_mask_fractions: [0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85]
  
  # Logging and checkpointing
  log_interval: 10
  save_interval: 100
  checkpoint_dir: "./checkpoints"

data:
  # Dataset parameters
  dataset_path: null
  use_dummy_data: true
  num_samples: 128
  
  # Data preprocessing
  tokenizer_path: null
  max_length: 512
  padding: "right"
  
  # Data loading
  num_workers: 0
  pin_memory: true
  shuffle: true
