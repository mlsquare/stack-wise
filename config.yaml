# StackWise Configuration
# Hierarchical configuration with validation and defaults

model:
  # Model dimensions
  vocab_size: null  # Will be set from tokenizer
  d_model: 4096
  n_layers: 8
  n_heads: 32
  n_kv_heads: 8
  d_ff: 14336
  
  # Attention configuration
  attention_type: "standard"        # standard | gqa | mla | kernel
  attention_mode: "bidirectional"   # bidirectional | causal
  
  # MLA specific parameters
  mla_rq: 1024
  mla_rkv: 512
  
  # Kernel attention parameters
  kernel_type: "gaussian"           # gaussian | laplacian | uniform
  kernel_dim: 64
  
  # Normalization and MLP
  dropout: 0.0
  tie_embeddings: true
  
  # Positional encoding
  use_rope: true
  rope_theta: 10000.0
  
  # Mask-diffusion parameters
  mask_fraction_min: 0.15
  mask_fraction_max: 0.90
  special_mask_id: 4

training:
  # Training parameters
  lr: 1.0e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]
  batch_size: 4
  seq_len: 512
  max_steps: 200
  
  # Device and memory
  device: "cuda"
  gradient_checkpointing: false
  mixed_precision: false
  
  # Layer-wise training
  layerwise_training: true
  activation_cache_dir: "./cache"
  save_activations: true
  
  # Fusion and fine-tuning
  fusion_enabled: true
  joint_tuning_steps: 50
  fine_tune_mode: "clm"             # clm | mlm | diffusion
  
  # Logging and checkpointing
  log_interval: 10
  save_interval: 100
  checkpoint_dir: "./checkpoints"

data:
  # Dataset parameters
  dataset_path: null
  use_dummy_data: true
  num_samples: 128
  
  # Data preprocessing
  tokenizer_path: null
  max_length: 512
  padding: "right"
  
  # Data loading
  num_workers: 0
  pin_memory: true
  shuffle: true
