data: !!python/object:config.base.DataConfig
  dataset_path: null
  max_length: 64
  num_samples: 10000
  num_workers: 0
  padding: right
  pin_memory: true
  shuffle: true
  tokenizer_path: null
  use_dummy_data: true
model: !!python/object:config.base.ModelConfig
  architecture: !!python/object:config.base.ArchitectureConfig
    blocks_per_stack: 1
    n_stacks: 4
  attention_custom: !!python/object:config.base.AttentionConfig
    attention_mode: bidirectional
    attention_type: mha
    kernel_dim: 64
    kernel_type: linear
    mla_rkv: 512
    mla_rq: 1024
  attention_preset: bert_style
  d_ff: 512
  d_model: 128
  dropout: 0.0
  freeze_up_proj: true
  mask_fraction_max: 0.9
  mask_fraction_min: 0.15
  n_heads: 4
  n_kv_heads: 4
  rope_theta: 10000.0
  special_mask_id: 4
  tie_embeddings: true
  tokenizer_embedding:
    adapter_hidden_dim: null
    embedding_option: embed_tokens
    family: gpt2
    freeze_embeddings: true
  use_rope: true
  vocab_size: 1000
training: !!python/object:config.base.TrainingConfig
  batch_size: 16
  cache_dir: ./cache
  cache_mode: stack
  checkpoint_dir: ./checkpoints
  current_block_lr: 0.0001
  device: auto
  end_to_end_scope: stackwise
  epochs_per_stack: 1
  fine_tune_mode: clm
  gradient_checkpointing: false
  joint_tuning_steps: 50
  log_interval: 10
  mask_schedule_type: linear
  mask_token_id: 103
  max_mask_fraction: 0.9
  max_steps: 100
  min_mask_fraction: 0.15
  num_time_steps: 8
  optimizer: !!python/object:config.base.OptimizerConfig
    betas:
    - 0.9
    - 0.95
    custom_params: {}
    dampening: 0.0
    eps: 1.0e-08
    groups: []
    lr: 1e-4
    momentum: 0.9
    nesterov: false
    optimizer_type: AdamW
    use_groups: false
    weight_decay: 0.01
  progressive: !!python/object:config.base.ProgressiveConfig
    cache_activations: true
    enabled: true
    max_stacks: 4
    new_stack_precision: full
    target_stacks: 4
    time_interpretation: depth
    training_objective: mlm
    trunk_strategy: frozen
  qlora: !!python/object:config.base.QLoRAConfig
    alpha: 32
    alpha_pattern: constant
    configs: {}
    dropout: 0.1
    enabled: true
    lr: 1.0e-05
    mixed_precision: true
    progressive_alpha: 16
    progressive_enabled: false
    progressive_rank: 8
    rank: 16
    rank_pattern: constant
    strategy: simplified
  quantization_enabled: true
  quantization_type: fp16
  run_id: default_run
  save_interval: 100
  save_rack: false
  save_stacks: true
  seq_len: 512
  strategy: progressive
  time_step_mask_fractions:
  - 0.15
  - 0.25
  - 0.35
  - 0.45
  - 0.55
  - 0.65
  - 0.75
  - 0.85
  time_step_masking: true
  use_wandb: false
  wandb_entity: null
  wandb_notes: null
  wandb_project: stack-wise
  wandb_run_name: null
  wandb_tags: []
