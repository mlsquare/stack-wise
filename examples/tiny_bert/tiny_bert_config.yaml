# Tiny BERT Configuration
# A minimal BERT model for demonstration and testing

model:
  # Model architecture
  vocab_size: 1000
  d_model: 128
  n_heads: 4
  n_kv_heads: 4  # Same as n_heads for standard MHA (no GQA)
  d_ff: 512
  
  # Architecture configuration
  architecture:
    n_stacks: 4
    blocks_per_stack: 1
    
  # Attention configuration
  attention_preset: "bert_style"

training:
  # Basic training parameters
  batch_size: 16
  max_steps: 100
  
  # Optimizer configuration
  optimizer:
    lr: 1e-4
    weight_decay: 0.01
    betas: [0.9, 0.95]
  
  # Gradient settings
  gradient_checkpointing: false
  
  # Device settings
  device: "auto"
  
  # Training strategy: HOW to train
  strategy: "progressive"
  
  # End-to-end training scope: WHAT to train (only used when strategy="end_to_end")
  end_to_end_scope: "stackwise"
  
  # Progressive training configuration
  progressive:
    enabled: true
    max_stacks: 4
    target_stacks: 4
    trunk_strategy: "frozen"
    new_stack_precision: "full"
    cache_activations: true
    time_interpretation: "depth"
    training_objective: "mlm"
  
  # Caching configuration
  cache_dir: "./cache"
  cache_mode: "stack"
  
  # Saving configuration
  save_stacks: true
  save_rack: false
  
  # Mask-diffusion training
  min_mask_fraction: 0.15
  max_mask_fraction: 0.90
  mask_schedule_type: "linear"
  mask_token_id: 103

data:
  # Dataset parameters
  dataset_path: null
  use_dummy_data: true
  num_samples: 10000
  max_length: 64
  
  # Data preprocessing
  tokenizer_path: null
  padding: "right"
  
  # Data loading
  num_workers: 0
  pin_memory: true
  shuffle: true