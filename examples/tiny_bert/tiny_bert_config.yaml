# Tiny BERT Configuration
# A minimal BERT model for demonstration and testing

model:
  # Model architecture
  vocab_size: 1000
  d_model: 128
  n_heads: 4
  d_ff: 512
  
  # Architecture configuration
  architecture:
    n_stacks: 4
    blocks_per_stack: 1
    d_model: 128
    d_ff: 512
    n_heads: 4
    
  # Attention configuration
  attention_type: "mha"
  attention_mode: "bidirectional"

training:
  # Basic training parameters
  lr: 1e-4
  batch_size: 16
  max_steps: 100
  weight_decay: 0.01
  
  # Optimizer settings
  betas: [0.9, 0.95]
  
  # Gradient settings
  gradient_checkpointing: false
  mixed_precision: false
  
  # Device settings
  device: "auto"
  
  # Layer-wise training
  layerwise_training: true
  activation_cache_dir: "./cache"
  save_activations: true
  
  # Caching configuration
  cache_mode: "layerwise"
  fusion_evaluation: false
  save_fused_checkpoints: false
  
  # Mask-diffusion training
  min_mask_fraction: 0.15
  max_mask_fraction: 0.90
  mask_schedule_type: "linear"
  mask_token_id: 103

data:
  # Dataset parameters
  dataset_path: null
  use_dummy_data: true
  num_samples: 10000
  max_length: 64
  
  # Data preprocessing
  tokenizer_path: null
  padding: "right"
  
  # Data loading
  num_workers: 0
  pin_memory: true
  shuffle: true