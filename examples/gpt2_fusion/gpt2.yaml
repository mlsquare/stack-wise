# GPT-2 Fusion Training Configuration
# Training GPT-2 with mask-diffusion objectives using FusionTrainer

model:
  # GPT-2 Architecture (Small)
  vocab_size: 50257  # GPT-2 vocab size
  d_model: 768       # GPT-2 small hidden size
  n_layers: 12       # GPT-2 small layers
  n_heads: 12        # GPT-2 small attention heads
  n_kv_heads: 12     # Standard attention (no GQA)
  d_ff: 3072        # 4 * d_model
  
  # GPT-2 specific settings
  attention_type: "standard"
  attention_mode: "causal"  # GPT-2 uses causal attention
  use_rope: false           # GPT-2 uses learned positional embeddings
  tie_embeddings: true      # GPT-2 ties input/output embeddings
  
  # Normalization and MLP
  dropout: 0.1
  rope_theta: 10000.0
  
  # Mask-diffusion parameters
  mask_fraction_min: 0.1
  mask_fraction_max: 0.99
  special_mask_id: 50256  # GPT-2 pad token
  
  # Tokenizer and embedding configuration
  tokenizer_embedding:
    family: "gpt2"
    embedding_option: "embed_tokens"
    freeze_embeddings: false  # Train embeddings
    adapter_hidden_dim: null

training:
  # Training parameters
  lr: 5.0e-4
  weight_decay: 0.01
  betas: [0.9, 0.95]
  batch_size: 8
  seq_len: 512
  max_steps: 1000
  
  # Device and memory
  device: "cuda"
  gradient_checkpointing: false
  mixed_precision: true
  
  # Fusion training
  mode: "fused"
  fusion_mode: "frozen"  # Start with frozen backbone
  total_blocks: 3       # 3 blocks of 4 layers each
  block_size: 4
  
  # Run identification
  run_id: "gpt2_fusion_training"
  
  # QLoRA and quantization settings
  qlora_enabled: true
  qlora_lr: 1.0e-5
  current_block_lr: 5.0e-4
  quantization_enabled: true
  quantization_type: "fp16"
  
  # Time-step-based masking
  time_step_masking: true
  num_time_steps: 12  # Match number of layers
  time_step_mask_fractions: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 0.99]
  
  # Mask-diffusion training
  min_mask_fraction: 0.1
  max_mask_fraction: 0.99
  mask_schedule_type: "linear"
  mask_token_id: 50256  # GPT-2 pad token
  epochs_per_layer: 1
  learning_rate: 5.0e-4
  
  # Logging and checkpointing
  log_interval: 10
  save_interval: 100
  checkpoint_dir: "./checkpoints/gpt2_fusion"

data:
  # Dataset parameters
  dataset_path: "./data/english_corpus_10k.json"
  use_dummy_data: false
  num_samples: 10000
  
  # Data preprocessing
  tokenizer_path: "gpt2"
  max_length: 512
  padding: "right"
  
  # Data loading
  num_workers: 4
  pin_memory: true
  shuffle: true
