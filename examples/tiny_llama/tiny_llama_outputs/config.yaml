data:
  dataset_path: null
  max_length: 128
  num_samples: 15000
  num_workers: 0
  padding: left
  pin_memory: true
  shuffle: true
  tokenizer_path: null
  use_dummy_data: true
model:
  architecture: !!python/object:config.base.ArchitectureConfig
    blocks_per_stack: 1
    n_stacks: 6
  attention_custom: !!python/object:config.base.AttentionConfig
    attention_mode: bidirectional
    attention_type: mha
    kernel_dim: 64
    kernel_type: linear
    mla_rkv: 512
    mla_rq: 1024
  attention_preset: gpt_style
  d_ff: 1024
  d_model: 256
  dropout: 0.0
  freeze_up_proj: false
  mask_fraction_max: 0.9
  mask_fraction_min: 0.15
  n_heads: 8
  n_kv_heads: 4
  rope_theta: 10000.0
  special_mask_id: 4
  tie_embeddings: true
  tokenizer_embedding:
    adapter_hidden_dim: null
    embedding_option: embed_tokens
    family: gpt2
    freeze_embeddings: true
  use_rope: true
  vocab_size: 32000
training:
  batch_size: 8
  cache_dir: ./cache
  cache_mode: stack
  checkpoint_dir: ./checkpoints
  current_block_lr: 0.0001
  device: auto
  end_to_end_scope: stackwise
  epochs_per_stack: 1
  fine_tune_mode: clm
  gradient_checkpointing: false
  joint_tuning_steps: 50
  log_interval: 10
  mask_schedule_type: linear
  mask_token_id: null
  max_mask_fraction: 0.0
  max_steps: 200
  min_mask_fraction: 0.0
  num_time_steps: 8
  optimizer: !!python/object:config.base.OptimizerConfig
    betas:
    - 0.9
    - 0.95
    custom_params: {}
    dampening: 0.0
    eps: 1.0e-08
    groups: []
    lr: 2e-4
    momentum: 0.9
    nesterov: false
    optimizer_type: AdamW
    use_groups: false
    weight_decay: 0.01
  progressive: !!python/object:config.base.ProgressiveConfig
    cache_activations: true
    enabled: true
    max_stacks: 6
    new_stack_precision: full
    target_stacks: 6
    time_interpretation: depth
    training_objective: clm
    trunk_strategy: frozen
  qlora: !!python/object:config.base.QLoRAConfig
    alpha: 32
    alpha_pattern: constant
    configs: {}
    dropout: 0.1
    enabled: true
    lr: 1.0e-05
    mixed_precision: true
    progressive_alpha: 16
    progressive_enabled: false
    progressive_rank: 8
    rank: 16
    rank_pattern: constant
    strategy: simplified
  quantization_enabled: true
  quantization_type: fp16
  run_id: default_run
  save_interval: 100
  save_rack: false
  save_stacks: true
  seq_len: 512
  strategy: progressive
  time_step_mask_fractions:
  - 0.15
  - 0.25
  - 0.35
  - 0.45
  - 0.55
  - 0.65
  - 0.75
  - 0.85
  time_step_masking: true
